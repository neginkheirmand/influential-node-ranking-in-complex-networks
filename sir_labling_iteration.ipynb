{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import ndlib.models.ModelConfig as mc\n",
    "import ndlib.models.epidemics as ep\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graph_paths(dataset_dir= \"./datasets/\"):\n",
    "    graph_list = []\n",
    "    for dirpath, _, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            try:\n",
    "                if filename.endswith(\".edges\"):\n",
    "                    file_path = os.path.join(dirpath, filename) \n",
    "                    graph_list.append((file_path, os.path.splitext(filename)[0]))\n",
    "            except Exception as e: \n",
    "                print(e, f'{filename}')\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def get_sir_graph_paths(net_name, num_b=3,  result_path = './datasets/SIR_Results/'):\n",
    "    paths= []\n",
    "    for i in range(num_b):\n",
    "        sir_dir =os.path.join(result_path, net_name)\n",
    "        sir_dir = os.path.join(sir_dir, f'{i}.csv')\n",
    "        paths.append(sir_dir)\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SIR(G, infected, B_values, gama=1.0, num_iterations=100, num_steps=200):\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    affected_scales = {}\n",
    "    infected_scales = {}\n",
    "    for B in B_values:\n",
    "        recovered_sum = 0  # To store the sum of recovered nodes across all iterations\n",
    "        infected_sum = 0\n",
    "\n",
    "        # Store trends for plotting\n",
    "        trends = []\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            # Initialize the SIR model\n",
    "            model = ep.SIRModel(G)\n",
    "            \n",
    "            # Configuration setup\n",
    "            config = mc.Configuration()\n",
    "            config.add_model_parameter('beta', B)  # Set infection rate to current B\n",
    "            config.add_model_parameter('gamma', gama)  # Recovery probability = 1\n",
    "            # config.add_model_initial_configuration(\"Infected\",  {0: 1})  # Start with node 0 infected\n",
    "            config.add_model_initial_configuration(\"Infected\",  infected)  \n",
    "            \n",
    "            # Set the model configuration\n",
    "            model.set_initial_status(config)\n",
    "            \n",
    "            \n",
    "            iteration = None\n",
    "            # Run the model until all nodes are either recovered or susceptible\n",
    "            for step in range(num_steps):  # Maximum 200 steps\n",
    "                iteration = model.iteration()\n",
    "                trends.append(model.build_trends([iteration]))\n",
    "                \n",
    "                # Check if all nodes are either recovered or susceptible (no infected nodes left)\n",
    "                if iteration['node_count'][1] == 0:  # Index 1 corresponds to 'Infected'\n",
    "                    break  # Exit the loop if no infected nodes remain\n",
    "\n",
    "            # Get the final state after the infection spread\n",
    "            final_state = iteration['node_count']\n",
    "            recovered_nodes = final_state[2]  # Index 2 represents 'Recovered' nodes\n",
    "            recovered_sum += recovered_nodes\n",
    "            infected_sum += final_state[1]# Index 1 represents 'inffected' nodes\n",
    "        \n",
    "        # Calculate the affected scale for the current B\n",
    "        affected_scale = recovered_sum / (num_iterations * num_nodes)\n",
    "        affected_scales[B] = round(affected_scale, 6)\n",
    "        infected_scales[B] = infected_sum \n",
    "    return affected_scales, infected_scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sir_vs_iterations(net_name, G, infected_nodes, B_values, gama, num_iterations_list, num_steps=200):\n",
    "    execution_times = {str(num_iterations): [] for num_iterations in num_iterations_list}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for node in infected_nodes:\n",
    "        infected = {node: 1}\n",
    "        average_sir_values = []\n",
    "\n",
    "        for num_iterations in num_iterations_list:\n",
    "            # Measure the start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run the SIR simulation\n",
    "            affected_scales, _ = SIR(G, infected, B_values, gama, num_iterations, num_steps)\n",
    "            \n",
    "            # Measure the end time\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time  # Duration in seconds\n",
    "\n",
    "            # Save the execution time\n",
    "            execution_times[str(num_iterations)].append(duration)\n",
    "\n",
    "            average_sir_value = sum(affected_scales.values()) / len(B_values)\n",
    "            average_sir_values.append(average_sir_value)\n",
    "\n",
    "        plt.plot(num_iterations_list, average_sir_values, marker='o', label=f\"Node {node}\")\n",
    "\n",
    "    plt.title(f\"SIR Value over Iterations: {net_name}\")\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Average SIR Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(f\"./sir_labeling/images/iterations/sir_val_over_iter_{net_name}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate average execution times for each num_iterations and return\n",
    "    avg_execution_times = {num_iterations: round(np.mean(times), 2) for num_iterations, times in execution_times.items()}\n",
    "    return avg_execution_times\n",
    "\n",
    "\n",
    "\n",
    "def choose_random_node(G):\n",
    "    return random.choice(list(G.nodes))\n",
    "\n",
    "\n",
    "def get_B_Value(G, num_b=3):\n",
    "    # Get the mean degree (k) of the graph\n",
    "    degrees = [deg for _, deg in G.degree()]\n",
    "    \n",
    "    # First moment (mean degree)\n",
    "    mean_degree = np.mean(degrees)\n",
    "\n",
    "    # Second moment (mean of squared degrees)\n",
    "    mean_degree_squared = np.mean([deg**2 for deg in degrees])\n",
    "\n",
    "    # Epidemic threshold (B_Threshold)\n",
    "    B_Threshold = mean_degree / (mean_degree_squared - mean_degree)\n",
    "    # Range of B values\n",
    "    B_values = np.linspace(1 * B_Threshold, 2 * B_Threshold, num_b)\n",
    "    # Use numpy's round function\n",
    "    B_values = np.round(B_values, 3)\n",
    "    B_values = B_values.tolist()\n",
    "    return B_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS\n",
    "\n",
    "GOOD for showing why there was impovement from 100(previous run's results) and the 200 that the paper used:\n",
    "\n",
    "ba_edgelist_1000_4\n",
    "\n",
    "ba_edgelist_2000_10\n",
    "\n",
    "ba_edgelist_2000_20\n",
    "\n",
    "ba_edgelist_3000_10\n",
    "\n",
    "ba_edgelist_3000_20\n",
    "\n",
    "ba_edgelist_exp1_6000_4\n",
    "\n",
    "\n",
    "\n",
    "BAD for ... :\n",
    "\n",
    "ba_edgelist_exp1_8000_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ba_edgelist_1000_10, already processed.\n",
      "Processing ba_edgelist_1000_20...\n",
      "Selected nodes: [783, 625, 332, 812, 525, 244, 530, 780]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# File path for execution times\n",
    "execution_times_file = \"all_execution_times.json\"\n",
    "\n",
    "# Load execution times if the file exists\n",
    "if os.path.exists(execution_times_file):\n",
    "    with open(execution_times_file, \"r\") as f:\n",
    "        all_execution_times = json.load(f)\n",
    "else:\n",
    "    all_execution_times = {}\n",
    "\n",
    "graph_list = get_graph_paths()\n",
    "\n",
    "# Example usage\n",
    "for graph in graph_list:\n",
    "    G_path = graph[0]\n",
    "    net_name = graph[1]\n",
    "\n",
    "    # Skip graphs that are already in all_execution_times\n",
    "    if net_name in all_execution_times:\n",
    "        print(f\"Skipping {net_name}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing {net_name}...\")\n",
    "    \n",
    "    G = nx.read_edgelist(G_path, comments=\"%\", nodetype=int)\n",
    "    \n",
    "    # Choose 8 random nodes\n",
    "    infected_nodes = [choose_random_node(G) for _ in range(8)]\n",
    "    print(\"Selected nodes:\", infected_nodes)\n",
    "\n",
    "    B_values = get_B_Value(G)\n",
    "    gama = 1.0\n",
    "    num_iterations_list = [10, 50, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "    # Analyze and get average execution times\n",
    "    avg_execution_times = analyze_sir_vs_iterations(net_name, G, infected_nodes, B_values, gama, num_iterations_list)\n",
    "\n",
    "    # Add to the overall execution times\n",
    "    all_execution_times[net_name] = avg_execution_times\n",
    "\n",
    "    # Save execution times to JSON after each graph is processed\n",
    "    with open(execution_times_file, \"w\") as json_file:\n",
    "        json.dump(all_execution_times, json_file, indent=4)\n",
    "        print(f\"Execution times for {net_name} saved to {execution_times_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "`num_iterations_list = [10, 50, 100, 200, 300, 400, 500, 1000]   \n",
    "`    \n",
    "\n",
    "- you may wonder why i removed the 1000 element of the list, honey running this simulation took 2 minutes and 15 seconds for the smallest of my graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
