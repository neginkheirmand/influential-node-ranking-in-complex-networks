{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ba_graph_paths(dataset_dir= \"./datasets/\"):\n",
    "    graph_list = []\n",
    "    for dirpath, _, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            try:\n",
    "                if filename.startswith(\"ba_edgelist\") and filename.endswith(\".edges\"):\n",
    "                    file_path = os.path.join(dirpath, filename) \n",
    "                    graph_list.append((file_path, os.path.splitext(filename)[0]))\n",
    "            except Exception as e: \n",
    "                print(e, f'{filename}')\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "\n",
    "def get_graph_paths(dataset_dir= \"./datasets/\"):\n",
    "    graph_list = []\n",
    "    for dirpath, _, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            try:\n",
    "                if filename.endswith(\".edges\"):\n",
    "                    file_path = os.path.join(dirpath, filename) \n",
    "                    graph_list.append((file_path, os.path.splitext(filename)[0]))\n",
    "            except Exception as e: \n",
    "                print(e, f'{filename}')\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def get_sir_graph_paths(net_name, num_b=3,  result_path = './datasets/SIR_Results/'):\n",
    "    paths= []\n",
    "    for i in range(num_b):\n",
    "        sir_dir =os.path.join(result_path, net_name)\n",
    "        sir_dir = os.path.join(sir_dir, f'{i}.csv')\n",
    "        paths.append(sir_dir)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def get_previously_sim_values(sir_graph_path):\n",
    "    try:\n",
    "        df = pd.read_csv(sir_graph_path)\n",
    "        values = df['Node'].tolist()\n",
    "        return values\n",
    "    except OSError as e:\n",
    "        return []\n",
    "\n",
    "def getTotalNumNodes(net_name):\n",
    "    graph_list = get_graph_paths()\n",
    "    path = ''\n",
    "    for tmp in graph_list:\n",
    "        if tmp[1]==net_name:\n",
    "            path = tmp[0]\n",
    "            \n",
    "    G = nx.read_edgelist(path, comments=\"%\", nodetype=int)\n",
    "    return G.number_of_nodes()\n",
    "\n",
    "\n",
    "def getSimNumNodes(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    temp = [  len(get_previously_sim_values(path)) for path in sir_paths ]\n",
    "    return (temp)\n",
    "\n",
    "\n",
    "def getSortValue(net_edges_path):\n",
    "    net_edges_path=net_edges_path[0]\n",
    "    G = nx.read_edgelist(net_edges_path, comments=\"%\", nodetype=int)\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_degree = 2 * num_edges / num_nodes if num_nodes > 0 else 0\n",
    "    return num_nodes+avg_degree\n",
    "\n",
    "def getSubList(graphs, index, step):\n",
    "    sublits = []\n",
    "    for i in range(index, len(graphs), step):\n",
    "        sublits.append(graphs[i])\n",
    "    return sublits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_has_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    duplicates = []\n",
    "    for sir_graph_path in sir_paths:\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(sir_graph_path)\n",
    "            # Check for duplicate Node values\n",
    "            duplicate = df[df.duplicated(subset='Node', keep=False)]['Node'].unique()\n",
    "            if len(duplicate)>0:\n",
    "                duplicates.append(duplicate)\n",
    "            else:\n",
    "                duplicates.append([])\n",
    "                \n",
    "        except OSError as e:\n",
    "            return []\n",
    "    union_result = set(duplicates[0]).union(duplicates[1], duplicates[2])\n",
    "    union_result = list(union_result)\n",
    "    return True if len (union_result)>0 else False\n",
    "\n",
    "def get_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    duplicates = []\n",
    "    for sir_graph_path in sir_paths:\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(sir_graph_path)\n",
    "            # Check for duplicate Node values\n",
    "            duplicate = df[df.duplicated(subset='Node', keep=False)]['Node'].unique()\n",
    "            if len(duplicate)>0:\n",
    "                duplicates.append(duplicate)\n",
    "            else:\n",
    "                duplicates.append([])\n",
    "                \n",
    "        except OSError as e:\n",
    "            return []\n",
    "    union_result = set(duplicates[0]).union(duplicates[1], duplicates[2])\n",
    "    union_result = list(union_result)\n",
    "    return union_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./datasets/BA\\\\ba_edgelist_1000_10.edges', 'ba_edgelist_1000_10')\n",
      "('./datasets/BA\\\\ba_edgelist_1000_20.edges', 'ba_edgelist_1000_20')\n",
      "('./datasets/BA\\\\ba_edgelist_1000_4.edges', 'ba_edgelist_1000_4')\n",
      "('./datasets/BA\\\\ba_edgelist_2000_10.edges', 'ba_edgelist_2000_10')\n",
      "('./datasets/BA\\\\ba_edgelist_2000_20.edges', 'ba_edgelist_2000_20')\n",
      "('./datasets/BA\\\\ba_edgelist_2000_4.edges', 'ba_edgelist_2000_4')\n",
      "('./datasets/BA\\\\ba_edgelist_3000_10.edges', 'ba_edgelist_3000_10')\n",
      "('./datasets/BA\\\\ba_edgelist_3000_20.edges', 'ba_edgelist_3000_20')\n",
      "('./datasets/BA\\\\ba_edgelist_3000_4.edges', 'ba_edgelist_3000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6')\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "temp = get_ba_graph_paths()\n",
    "for i in temp:\n",
    "    print(i)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6')\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "rmv= ['ba_edgelist_1000_10', \"ba_edgelist_1000_20\", \"ba_edgelist_1000_4\", \"ba_edgelist_2000_10\", \"ba_edgelist_2000_20\", \"ba_edgelist_2000_4\", \"ba_edgelist_3000_10\", \"ba_edgelist_3000_20\", \"ba_edgelist_3000_4\"]\n",
    "temp = [x for x in temp if x[1] not in rmv]\n",
    "for i in temp:\n",
    "    print(i)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6')]\n",
      "[('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4')]\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(temp)\n",
    "list.sort(temp, key=getSortValue)\n",
    "print(temp)\n",
    "ba_mch_0 = getSubList(temp, 0, 3)\n",
    "ba_mch_1 = getSubList(temp, 1, 3)\n",
    "ba_mch_2 = getSubList(temp, 2, 3)\n",
    "print(len(ba_mch_0))\n",
    "print(len(ba_mch_1))\n",
    "print(len(ba_mch_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba_mch_0:\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4')\n",
      "ba_mch_1:\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4')\n",
      "ba_mch_2:\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4')\n",
      "('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4')\n"
     ]
    }
   ],
   "source": [
    "print('ba_mch_0:')\n",
    "for i in ba_mch_0:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "print('ba_mch_1:')\n",
    "for i in ba_mch_1:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "print('ba_mch_2:')\n",
    "for i in ba_mch_2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "di = {}\n",
    "di['BA_mch_0'] = [item[1] for item in ba_mch_0]\n",
    "# di['BA_mch_0'].append('ChicagoRegional')\n",
    "# di['BA_mch_0'].append('ia-crime-moreno')\n",
    "di['BA_mch_0'].append('p2p-Gnutella04')\n",
    "# di['BA_mch_0'].append('jazz')\n",
    "di['BA_mch_0'].append('LastFM')\n",
    "\n",
    "\n",
    "di['BA_mch_1'] = [item[1] for item in ba_mch_1]\n",
    "# di['BA_mch_1'].append('sex')\n",
    "di['BA_mch_1'].append('powergrid')\n",
    "di['BA_mch_1'].append('vidal')\n",
    "di['BA_mch_1'].append('politician_edges')\n",
    "\n",
    "\n",
    "\n",
    "di['BA_mch_2'] = [item[1] for item in ba_mch_2]\n",
    "# di['BA_mch_2'].append('maybe-PROTEINS-full')\n",
    "# di['BA_mch_2'].append('arenas-pgp')\n",
    "\n",
    "# di['negin_mch'] = ['CA-GrQc', 'CA-HepTh', 'faa', 'facebook_combined', 'figeys', 'email', 'NS', 'Peh_edge', 'Stelzl', 'tvshow_edges', 'web-EPA']\n",
    "di['negin_mch'] = ['CA-GrQc', 'CA-HepTh', 'facebook_combined', 'figeys', 'email', 'Peh_edge', 'Stelzl', 'tvshow_edges', 'web-EPA', 'arenas-pgp']\n",
    "di['mhd_mch'] = [ 'ChicagoRegional', 'ia-crime-moreno', 'maybe-PROTEINS-full', 'NS', 'faa',                  'jazz', 'sex']   #TODO: THESE TWO LAST ONES SHOULD BE DONE LATERd\n",
    "\n",
    "write_to_mch_json = False\n",
    "if write_to_mch_json:\n",
    "    with open('machine.json', 'w') as f:\n",
    "        json.dump(di, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BA_mch_0': ['ba_edgelist_exp1_3000_4', 'ba_edgelist_exp3_4000_6', 'ba_edgelist_exp1_6000_4', 'p2p-Gnutella04', 'LastFM'], 'BA_mch_1': ['ba_edgelist_exp2_2000_10', 'ba_edgelist_exp3_4000_2', 'ba_edgelist_exp3_4000_10', 'ba_edgelist_exp1_7000_4', 'vidal', 'politician_edges'], 'BA_mch_2': ['ba_edgelist_exp2_2000_20', 'ba_edgelist_exp1_4000_4', 'ba_edgelist_exp1_5000_4', 'ba_edgelist_exp2_2000_4'], 'negin_mch': ['CA-GrQc', 'CA-HepTh', 'facebook_combined', 'figeys', 'email', 'Peh_edge', 'Stelzl', 'tvshow_edges', 'web-EPA', 'arenas-pgp', 'powergrid', 'ba_edgelist_exp1_8000_4'], 'mhd_mch': ['jazz', 'NS', 'faa', 'ba_edgelist_1000_4', 'ChicagoRegional', 'ia-crime-moreno', 'maybe-PROTEINS-full', 'sex']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('machine.json', 'r') as file:\n",
    "    di = json.load(file)\n",
    "\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_mch_0 : \n",
      "1 . ba_edgelist_exp1_3000_4   process: [3000, 3000, 3000] / 3000 completed:  True  sorted: True\n",
      "2 . ba_edgelist_exp3_4000_6   process: [4000, 4000, 4000] / 4000 completed:  True  sorted: True\n",
      "3 . ba_edgelist_exp1_6000_4   process: [6000, 6000, 6000] / 6000 completed:  True  sorted: True\n",
      "4 . p2p-Gnutella04   process: [2489, 2489, 2489] / 10876 completed:  False  sorted: True\n",
      "5 . LastFM   process: [7987, 7987, 7987] / 7624 completed:  True  sorted: True\n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [np.int64(5502), np.int64(5503), np.int64(5504), np.int64(5505), np.int64(5506), np.int64(5507), np.int64(5508), np.int64(5509), np.int64(5510), np.int64(5511), np.int64(5512), np.int64(5513), np.int64(5514), np.int64(5515), np.int64(5516), np.int64(5517), np.int64(5518), np.int64(5519), np.int64(5520), np.int64(5521), np.int64(5522), np.int64(5523), np.int64(5524), np.int64(5525), np.int64(5526), np.int64(5527), np.int64(5528), np.int64(5529), np.int64(5530), np.int64(5531), np.int64(5532), np.int64(5533), np.int64(5534), np.int64(5535), np.int64(5536), np.int64(5537), np.int64(5538), np.int64(5539), np.int64(5540), np.int64(5541), np.int64(5542), np.int64(5543), np.int64(5544), np.int64(5545), np.int64(5546), np.int64(5547), np.int64(5548), np.int64(5549), np.int64(5550), np.int64(5551), np.int64(5552), np.int64(5553), np.int64(5554), np.int64(5555), np.int64(5556), np.int64(5557), np.int64(5558), np.int64(5559), np.int64(5560), np.int64(5561), np.int64(5562), np.int64(5563), np.int64(5564), np.int64(5565), np.int64(5566), np.int64(5567), np.int64(5568), np.int64(5569), np.int64(5570), np.int64(5571), np.int64(5572), np.int64(5573), np.int64(5574), np.int64(5575), np.int64(5576), np.int64(5577), np.int64(5578), np.int64(5579), np.int64(5580), np.int64(5581), np.int64(5582), np.int64(5583), np.int64(5584), np.int64(5585), np.int64(5586), np.int64(5587), np.int64(5588), np.int64(5589), np.int64(5590), np.int64(5591), np.int64(5592), np.int64(5593), np.int64(5594), np.int64(5595), np.int64(5596), np.int64(5597), np.int64(5598), np.int64(5599), np.int64(5600), np.int64(5601), np.int64(5602), np.int64(5603), np.int64(5604), np.int64(5605), np.int64(5606), np.int64(5607), np.int64(5608), np.int64(5609), np.int64(5610), np.int64(5611), np.int64(5612), np.int64(5613), np.int64(5614), np.int64(5615), np.int64(5616), np.int64(5617), np.int64(5618), np.int64(5619), np.int64(5620), np.int64(5621), np.int64(5622), np.int64(5623), np.int64(5624), np.int64(5625), np.int64(5626), np.int64(5627), np.int64(5628), np.int64(5629), np.int64(5630), np.int64(5631), np.int64(5632), np.int64(5633), np.int64(5634), np.int64(5635), np.int64(5636), np.int64(5637), np.int64(5638), np.int64(5639), np.int64(5640), np.int64(5641), np.int64(5642), np.int64(5643), np.int64(5644), np.int64(5645), np.int64(5646), np.int64(5647), np.int64(5648), np.int64(5649), np.int64(5650), np.int64(5651), np.int64(5652), np.int64(5653), np.int64(5654), np.int64(5655), np.int64(5656), np.int64(5657), np.int64(5658), np.int64(5659), np.int64(5660), np.int64(5661), np.int64(5662), np.int64(5663), np.int64(5664), np.int64(5665), np.int64(5666), np.int64(5667), np.int64(5668), np.int64(5669), np.int64(5670), np.int64(5671), np.int64(5672), np.int64(5673), np.int64(5674), np.int64(5675), np.int64(5676), np.int64(5677), np.int64(5678), np.int64(5679), np.int64(5680), np.int64(5681), np.int64(5682), np.int64(5683), np.int64(5684), np.int64(5685), np.int64(5686), np.int64(5687), np.int64(5688), np.int64(5689), np.int64(5690), np.int64(5691), np.int64(5692), np.int64(5693), np.int64(5694), np.int64(5695), np.int64(5696), np.int64(5697), np.int64(5698), np.int64(5699), np.int64(5700), np.int64(5701), np.int64(5702), np.int64(5703), np.int64(5704), np.int64(5705), np.int64(5706), np.int64(5707), np.int64(5708), np.int64(5709), np.int64(5710), np.int64(5711), np.int64(5712), np.int64(5713), np.int64(5714), np.int64(5715), np.int64(5716), np.int64(5717), np.int64(5718), np.int64(5719), np.int64(5720), np.int64(5721), np.int64(5722), np.int64(5723), np.int64(5724), np.int64(5725), np.int64(5726), np.int64(5727), np.int64(5728), np.int64(5729), np.int64(5730), np.int64(5731), np.int64(5732), np.int64(5733), np.int64(5734), np.int64(5735), np.int64(5736), np.int64(5737), np.int64(5738), np.int64(5739), np.int64(5740), np.int64(5741), np.int64(5742), np.int64(5743), np.int64(5744), np.int64(5745), np.int64(5746), np.int64(5747), np.int64(5748), np.int64(5749), np.int64(5750), np.int64(5751), np.int64(5752), np.int64(5753), np.int64(5754), np.int64(5755), np.int64(5756), np.int64(5757), np.int64(5758), np.int64(5759), np.int64(5760), np.int64(5761), np.int64(5762), np.int64(5763), np.int64(5764), np.int64(5765), np.int64(5766), np.int64(5767), np.int64(5768), np.int64(5769), np.int64(5770), np.int64(5771), np.int64(5772), np.int64(5773), np.int64(5774), np.int64(5775), np.int64(5776), np.int64(5777), np.int64(5778), np.int64(5779), np.int64(5780), np.int64(5781), np.int64(5782), np.int64(5783), np.int64(5784), np.int64(5785), np.int64(5786), np.int64(5787), np.int64(5788), np.int64(5789), np.int64(5790), np.int64(5791), np.int64(5792), np.int64(5793), np.int64(5794), np.int64(5795), np.int64(5796), np.int64(5797), np.int64(5798), np.int64(5799), np.int64(5800), np.int64(5801), np.int64(5802), np.int64(5803), np.int64(5804), np.int64(5805), np.int64(5806), np.int64(5807), np.int64(5808), np.int64(5809), np.int64(5810), np.int64(5811), np.int64(5812), np.int64(5813), np.int64(5814), np.int64(5815), np.int64(5816), np.int64(5817), np.int64(5818), np.int64(5819), np.int64(5820), np.int64(5821), np.int64(5822), np.int64(5823), np.int64(5824), np.int64(5825), np.int64(5826), np.int64(5827), np.int64(5828), np.int64(5829), np.int64(5830), np.int64(5831), np.int64(5832), np.int64(5833), np.int64(5834), np.int64(5835), np.int64(5836), np.int64(5837), np.int64(5838), np.int64(5839), np.int64(5840), np.int64(5841), np.int64(5842), np.int64(5843), np.int64(5844), np.int64(5845), np.int64(5846), np.int64(5847), np.int64(5848), np.int64(5849), np.int64(5850), np.int64(5851), np.int64(5852), np.int64(5853), np.int64(5854), np.int64(5855), np.int64(5856), np.int64(5857), np.int64(5858), np.int64(5859), np.int64(5860), np.int64(5861), np.int64(5862), np.int64(5863), np.int64(5864)]\n",
      "SIM NUM NODES / TOTAL NUM NODES:  23476  /  31500\n",
      "percentage:  74.52698412698413\n",
      "COMPLETED:  ['ba_edgelist_exp1_3000_4', 'ba_edgelist_exp3_4000_6', 'ba_edgelist_exp1_6000_4', 'LastFM']\n",
      "INCOMPLETED:  ['p2p-Gnutella04']\n",
      "-----------------\n",
      "BA_mch_1 : \n",
      "1 . ba_edgelist_exp2_2000_10   process: [2000, 2000, 2000] / 2000 completed:  True  sorted: True\n",
      "2 . ba_edgelist_exp3_4000_2   process: [4000, 4000, 4000] / 4000 completed:  True  sorted: True\n",
      "3 . ba_edgelist_exp3_4000_10   process: [4000, 4000, 4000] / 4000 completed:  True  sorted: True\n",
      "4 . ba_edgelist_exp1_7000_4   process: [7000, 7000, 7000] / 7000 completed:  True  sorted: True\n",
      "5 . vidal   process: [3133, 3133, 3133] / 3133 completed:  True  sorted: True\n",
      "6 . politician_edges   process: [5908, 5908, 5908] / 5908 completed:  True  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  26041  /  26041\n",
      "percentage:  100.0\n",
      "COMPLETED:  ['ba_edgelist_exp2_2000_10', 'ba_edgelist_exp3_4000_2', 'ba_edgelist_exp3_4000_10', 'ba_edgelist_exp1_7000_4', 'vidal', 'politician_edges']\n",
      "INCOMPLETED:  []\n",
      "-----------------\n",
      "BA_mch_2 : \n",
      "1 . ba_edgelist_exp2_2000_20   process: [2000, 2000, 2000] / 2000 completed:  True  sorted: True\n",
      "2 . ba_edgelist_exp1_4000_4   process: [4000, 4000, 4000] / 4000 completed:  True  sorted: True\n",
      "3 . ba_edgelist_exp1_5000_4   process: [5000, 5000, 5000] / 5000 completed:  True  sorted: True\n",
      "4 . ba_edgelist_exp2_2000_4   process: [2000, 2000, 2000] / 2000 completed:  True  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  13000  /  13000\n",
      "percentage:  100.0\n",
      "COMPLETED:  ['ba_edgelist_exp2_2000_20', 'ba_edgelist_exp1_4000_4', 'ba_edgelist_exp1_5000_4', 'ba_edgelist_exp2_2000_4']\n",
      "INCOMPLETED:  []\n",
      "-----------------\n",
      "negin_mch : \n",
      "1 . CA-GrQc   process: [5242, 5242, 5242] / 5242 completed:  True  sorted: True\n",
      "2 . CA-HepTh   process: [4386, 4386, 4386] / 9877 completed:  False  sorted: True\n",
      "3 . facebook_combined   process: [4039, 4039, 4039] / 4039 completed:  True  sorted: True\n",
      "4 . figeys   process: [2239, 2239, 2239] / 2239 completed:  True  sorted: True\n",
      "5 . email   process: [1133, 1133, 1133] / 1133 completed:  True  sorted: True\n",
      "6 . Peh_edge   process: [2426, 2426, 2426] / 2426 completed:  True  sorted: True\n",
      "7 . Stelzl   process: [1706, 1706, 1706] / 1706 completed:  True  sorted: True\n",
      "8 . tvshow_edges   process: [3892, 3892, 3892] / 3892 completed:  True  sorted: True\n",
      "9 . web-EPA   process: [4271, 4271, 4271] / 4271 completed:  True  sorted: True\n",
      "10 . arenas-pgp   process: [4873, 4873, 4873] / 10680 completed:  False  sorted: True\n",
      "11 . powergrid   process: [1558, 1558, 1558] / 4941 completed:  False  sorted: True\n",
      "12 . ba_edgelist_exp1_8000_4   process: [8000, 8000, 8000] / 8000 completed:  True  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  43765  /  58446\n",
      "percentage:  74.88108681517983\n",
      "COMPLETED:  ['CA-GrQc', 'facebook_combined', 'figeys', 'email', 'Peh_edge', 'Stelzl', 'tvshow_edges', 'web-EPA', 'ba_edgelist_exp1_8000_4']\n",
      "INCOMPLETED:  ['CA-HepTh', 'arenas-pgp', 'powergrid']\n",
      "-----------------\n",
      "mhd_mch : \n",
      "1 . jazz   process: [198, 198, 198] / 198 completed:  True  sorted: True\n",
      "2 . NS   process: [864, 864, 864] / 1461 completed:  False  sorted: True\n",
      "3 . faa   process: [472, 472, 472] / 1226 completed:  False  sorted: True\n",
      "4 . ba_edgelist_1000_4   process: [1000, 1000, 1000] / 1000 completed:  True  sorted: True\n",
      "5 . ChicagoRegional   process: [0, 0, 0] / 12979 completed:  False  sorted: True\n",
      "6 . ia-crime-moreno   process: [0, 0, 0] / 829 completed:  False  sorted: True\n",
      "7 . maybe-PROTEINS-full   process: [0, 0, 0] / 43466 completed:  False  sorted: True\n",
      "8 . sex   process: [0, 0, 0] / 10106 completed:  False  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  2534  /  71265\n",
      "percentage:  3.5557426506700343\n",
      "COMPLETED:  ['jazz', 'ba_edgelist_1000_4']\n",
      "INCOMPLETED:  ['NS', 'faa', 'ChicagoRegional', 'ia-crime-moreno', 'maybe-PROTEINS-full', 'sex']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "notsorted = [] \n",
    "noteq = []\n",
    "completed = []\n",
    "incomplete = []\n",
    "temp = False\n",
    "net_has_duplicate_name = []\n",
    "\n",
    "for mch in di.keys():\n",
    "    # if not mch=='BA_mch_1':\n",
    "    #     continue\n",
    "    print(mch, \": \")\n",
    "    i = 0\n",
    "\n",
    "    t_=0\n",
    "    s_=0\n",
    "    for net in di[mch]:\n",
    "        has_duplicates = get_has_duplicates(net)\n",
    "        temp = False\n",
    "        i+=1\n",
    "        simNodes = getSimNumNodes(net)\n",
    "        totalNumNodes = getTotalNumNodes(net)\n",
    "        t_+=totalNumNodes\n",
    "        s_+=simNodes[0]\n",
    "        sortd = sorted(get_previously_sim_values(get_sir_graph_paths(net)[0])) ==get_previously_sim_values(get_sir_graph_paths(net)[0])\n",
    "        if simNodes[0] >= totalNumNodes:\n",
    "            temp = True\n",
    "            completed.append(net)\n",
    "        else: \n",
    "            incomplete.append(net)\n",
    "        if not sortd :\n",
    "            # if mch == 'negin_mch':\n",
    "            notsorted.append(net)\n",
    "            print('### ', i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd, '### ')\n",
    "        elif not( simNodes[0]==simNodes[1] and simNodes[1]==simNodes[2]) :\n",
    "            noteq.append(net)\n",
    "            print('$$$ ', i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd, ('$$$ '))\n",
    "        else:\n",
    "            print( i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd)\n",
    "        if has_duplicates:\n",
    "            net_has_duplicate_name.append(net)\n",
    "            print(\"@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates: \", get_duplicates(net))\n",
    "    print(\"SIM NUM NODES / TOTAL NUM NODES: \",s_, ' / ', t_)\n",
    "    try:\n",
    "        print(\"percentage: \",(s_/ t_)*100)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"percentage: ZeroDivisionError  :\",s_, '/', t_ )\n",
    "\n",
    "    print(\"COMPLETED: \", completed)\n",
    "    print(\"INCOMPLETED: \", incomplete)\n",
    "\n",
    "    completed=[]\n",
    "    incomplete = []\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max value of column infected_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_infected_sum = float('-inf')  # Initialize to negative infinity\n",
    "for mch in di.keys():\n",
    "    # print(mch, \": \")\n",
    "    for net in di[mch]:\n",
    "        sir_paths = get_sir_graph_paths(net)\n",
    "        # print(sir_paths)\n",
    "        for file in sir_paths:\n",
    "            if file_exists(file):\n",
    "                df = pd.read_csv(file)\n",
    "                # print(df['Infected_sum'].max())\n",
    "                max_infected_sum = max(max_infected_sum, df['Infected_sum'].max())\n",
    "print(max_infected_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    for net in notsorted:\n",
    "        sir_paths = get_sir_graph_paths(net)\n",
    "        i = 0\n",
    "        for path in sir_paths:\n",
    "            i+=1\n",
    "            df = pd.read_csv(path)\n",
    "            df_sorted = df.sort_values(by='Node')\n",
    "            df_sorted.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for net in notsorted:\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE full duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LastFM']\n"
     ]
    }
   ],
   "source": [
    "print(net_has_duplicate_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_full_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    for sir_graph_path in sir_paths:\n",
    "        try:\n",
    "            # Load the CSV file with explicit data types\n",
    "            df = pd.read_csv(sir_graph_path, dtype={'Node': int, 'SIR': float, 'Infected_sum': int})\n",
    "            \n",
    "            # Remove fully duplicated rows, ensuring all columns are considered\n",
    "            df_cleaned = df.drop_duplicates(subset=['Node', 'SIR', 'Infected_sum'])\n",
    "            \n",
    "            # Overwrite the original CSV file with the cleaned data\n",
    "            df_cleaned.to_csv(sir_graph_path, index=False)\n",
    "            original_count = len(df)\n",
    "            cleaned_count = len(df_cleaned)\n",
    "            if original_count > cleaned_count:\n",
    "                print(f\" + File {sir_graph_path} updated, fully duplicated rows removed.\")\n",
    "            else:\n",
    "                print(f\" - no FULLY duplicate rows in File {sir_graph_path}.\")\n",
    "\n",
    "        except OSError as e:\n",
    "            print(\"Error reading file:\", e)\n",
    "\n",
    "if False:\n",
    "    for net in net_has_duplicate_name:\n",
    "        print(\"net: \", net)\n",
    "        remove_full_duplicates(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_node_values(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    for sir_graph_path in sir_paths:\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(sir_graph_path)\n",
    "            \n",
    "            # Group by 'Node' and calculate the mean for the other columns\n",
    "            df_avg = df.groupby('Node', as_index=False).mean()\n",
    "            df_avg['Infected_sum'] = df_avg['Infected_sum'].astype(int)\n",
    "            df_avg['SIR'] = df_avg['SIR'].round(7)\n",
    "            \n",
    "            # Overwrite the original CSV file with the averaged data\n",
    "            df_avg.to_csv(sir_graph_path, index=False)\n",
    "            \n",
    "            print(f\"CSV file {sir_graph_path} updated: Averaged rows for nodes with duplicate entries.\")\n",
    "            \n",
    "        except OSError as e:\n",
    "            print(f\"Error reading file: {e}\")\n",
    "            \n",
    "if False:\n",
    "    for net in net_has_duplicate_name:\n",
    "        print(\"net: \", net)\n",
    "        average_node_values(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_sir_precision_and_plot(csv_file_path, name):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    data = None\n",
    "    if file_exists(csv_file_path):\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    # Check if the 'SIR' column exists\n",
    "    if 'SIR' not in data.columns:\n",
    "        raise ValueError(\"The 'SIR' column does not exist in the provided CSV file.\")\n",
    "\n",
    "    # Extract the SIR values\n",
    "    sir_values = data['SIR']\n",
    "    \n",
    "    # Check the precision of each value (5 or 3 decimal places)\n",
    "    rounded_to_5 = sir_values.apply(lambda x: len(str(x).split(\".\")[1]) == 5 if \".\" in str(x) else False).sum()\n",
    "    rounded_to_3 = sir_values.apply(lambda x: len(str(x).split(\".\")[1]) == 3 if \".\" in str(x) else False).sum()\n",
    "    \n",
    "    \n",
    "    # if rounded_to_3 == 0 or (rounded_to_3 > 0 and rounded_to_5 > 0) :\n",
    "    if name=='maybe-PROTEINS-full' or name== 'ChicagoRegional':\n",
    "        return  0\n",
    "    \n",
    "    print(name)\n",
    "    # Print the results\n",
    "    print(f\"Values rounded to 5 digits after the point: {rounded_to_5}\")\n",
    "    print(f\"Values rounded to 3 digits after the point: {rounded_to_3}\")\n",
    "\n",
    "    # Plot the histogram of the SIR values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sir_values, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Histogram of Influential Scale value {name}\")\n",
    "    plt.xlabel(\"IS\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # plt.show()\n",
    "    # plt.savefig(f\"./sir_labeling/images/prec_3/hist_sir_prec3_{name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"./sir_labeling/images/prec_5/hist_sir_prec5_{name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return rounded_to_3\n",
    "\n",
    "# Example usage\n",
    "# check_sir_precision_and_plot(\"path_to_your_csv_file.csv\")\n",
    "if False:\n",
    "\n",
    "    sum_redo_nodes = 0\n",
    "    for mch in di.keys():\n",
    "        print(mch, \": \")\n",
    "        mch_sum=0\n",
    "        for net in di[mch]:\n",
    "            sir_paths = get_sir_graph_paths(net)\n",
    "            redo_nodes= check_sir_precision_and_plot(sir_paths[0], net)\n",
    "            sum_redo_nodes+=redo_nodes\n",
    "            mch_sum+=redo_nodes\n",
    "        print(f\"this machine {mch} has to do {mch_sum}\")\n",
    "    print(sum_redo_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sir_with_3_digit_precision(csv_file_path, output_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Check if the 'SIR' column exists\n",
    "    if 'SIR' not in data.columns:\n",
    "        raise ValueError(\"The 'SIR' column does not exist in the provided CSV file.\")\n",
    "\n",
    "    # Remove rows where 'SIR' values have 3 decimal places\n",
    "    filtered_data = data[~data['SIR'].apply(lambda x: len(str(x).split(\".\")[1]) == 3 if \".\" in str(x) else False)]\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_data.to_csv(output_file_path, index=False)\n",
    "    print(f\"Filtered data saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "if False:\n",
    "\n",
    "    for mch in di.keys():\n",
    "        for net in di[mch]:\n",
    "            sir_paths = get_sir_graph_paths(net)\n",
    "            if check_sir_precision_and_plot(sir_paths[0], net)>0:\n",
    "                remove_sir_with_3_digit_precision(sir_paths[0], sir_paths[0])\n",
    "                remove_sir_with_3_digit_precision(sir_paths[1], sir_paths[1])\n",
    "                remove_sir_with_3_digit_precision(sir_paths[2], sir_paths[2])\n",
    "        print(f\"this machine {mch} has to do {mch_sum}\")\n",
    "    print(sum_redo_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIR Result Convergence point of the different graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
