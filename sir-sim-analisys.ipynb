{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ba_graph_paths(dataset_dir= \"./datasets/\"):\n",
    "    graph_list = []\n",
    "    for dirpath, _, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            try:\n",
    "                if filename.startswith(\"ba_edgelist\") and filename.endswith(\".edges\"):\n",
    "                    file_path = os.path.join(dirpath, filename) \n",
    "                    graph_list.append((file_path, os.path.splitext(filename)[0]))\n",
    "            except Exception as e: \n",
    "                print(e, f'{filename}')\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def get_graph_paths(dataset_dir= \"./datasets/\"):\n",
    "    graph_list = []\n",
    "    for dirpath, _, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            try:\n",
    "                if filename.endswith(\".edges\"):\n",
    "                    file_path = os.path.join(dirpath, filename) \n",
    "                    graph_list.append((file_path, os.path.splitext(filename)[0]))\n",
    "            except Exception as e: \n",
    "                print(e, f'{filename}')\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def get_sir_graph_paths(net_name, num_b=3,  result_path = './datasets/SIR_Results/'):\n",
    "    paths= []\n",
    "    for i in range(num_b):\n",
    "        sir_dir =os.path.join(result_path, net_name)\n",
    "        sir_dir = os.path.join(sir_dir, f'{i}.csv')\n",
    "        paths.append(sir_dir)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def get_previously_sim_values(sir_graph_path):\n",
    "    try:\n",
    "        df = pd.read_csv(sir_graph_path)\n",
    "        values = df['Node'].tolist()\n",
    "        return values\n",
    "    except OSError as e:\n",
    "        return []\n",
    "\n",
    "def getTotalNumNodes(net_name):\n",
    "    graph_list = get_graph_paths()\n",
    "    path = ''\n",
    "    for tmp in graph_list:\n",
    "        if tmp[1]==net_name:\n",
    "            path = tmp[0]\n",
    "            \n",
    "    G = nx.read_edgelist(path, comments=\"%\", nodetype=int)\n",
    "    return G.number_of_nodes()\n",
    "\n",
    "\n",
    "def getSimNumNodes(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    temp = [  len(get_previously_sim_values(path)) for path in sir_paths ]\n",
    "    return (temp)\n",
    "\n",
    "\n",
    "def getSortValue(net_edges_path):\n",
    "    net_edges_path=net_edges_path[0]\n",
    "    G = nx.read_edgelist(net_edges_path, comments=\"%\", nodetype=int)\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_edges = G.number_of_edges()\n",
    "    avg_degree = 2 * num_edges / num_nodes if num_nodes > 0 else 0\n",
    "    return num_nodes+avg_degree\n",
    "\n",
    "def getSubList(graphs, index, step):\n",
    "    sublits = []\n",
    "    for i in range(index, len(graphs), step):\n",
    "        sublits.append(graphs[i])\n",
    "    return sublits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_has_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    sir_graph_path = sir_paths[0]\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(sir_graph_path)\n",
    "\n",
    "        # Check for duplicate Node values\n",
    "        duplicates = df[df.duplicated(subset='Node', keep=False)]\n",
    "\n",
    "        # Display the duplicates if any\n",
    "        if duplicates.empty:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    except OSError as e:\n",
    "        return []\n",
    "\n",
    "def get_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    sir_graph_path = sir_paths[0]\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(sir_graph_path)\n",
    "        # Check for duplicate Node values\n",
    "        duplicates = df[df.duplicated(subset='Node', keep=False)]['Node'].unique()\n",
    "\n",
    "        return duplicates.tolist() if len(duplicates) > 0 else []\n",
    "    except OSError as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./datasets/BA\\\\ba_edgelist_1000_10.edges', 'ba_edgelist_1000_10'), ('./datasets/BA\\\\ba_edgelist_1000_20.edges', 'ba_edgelist_1000_20'), ('./datasets/BA\\\\ba_edgelist_1000_4.edges', 'ba_edgelist_1000_4'), ('./datasets/BA\\\\ba_edgelist_2000_10.edges', 'ba_edgelist_2000_10'), ('./datasets/BA\\\\ba_edgelist_2000_20.edges', 'ba_edgelist_2000_20'), ('./datasets/BA\\\\ba_edgelist_2000_4.edges', 'ba_edgelist_2000_4'), ('./datasets/BA\\\\ba_edgelist_3000_10.edges', 'ba_edgelist_3000_10'), ('./datasets/BA\\\\ba_edgelist_3000_20.edges', 'ba_edgelist_3000_20'), ('./datasets/BA\\\\ba_edgelist_3000_4.edges', 'ba_edgelist_3000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6')]\n",
      "[('./datasets/BA\\\\ba_edgelist_1000_4.edges', 'ba_edgelist_1000_4'), ('./datasets/BA\\\\ba_edgelist_1000_10.edges', 'ba_edgelist_1000_10'), ('./datasets/BA\\\\ba_edgelist_1000_20.edges', 'ba_edgelist_1000_20'), ('./datasets/BA\\\\ba_edgelist_2000_4.edges', 'ba_edgelist_2000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_4.edges', 'ba_edgelist_exp2_2000_4'), ('./datasets/BA\\\\ba_edgelist_2000_10.edges', 'ba_edgelist_2000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_10.edges', 'ba_edgelist_exp2_2000_10'), ('./datasets/BA\\\\ba_edgelist_2000_20.edges', 'ba_edgelist_2000_20'), ('./datasets/BA_EXP\\\\ba_edgelist_exp2_2000_20.edges', 'ba_edgelist_exp2_2000_20'), ('./datasets/BA\\\\ba_edgelist_3000_4.edges', 'ba_edgelist_3000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_3000_4.edges', 'ba_edgelist_exp1_3000_4'), ('./datasets/BA\\\\ba_edgelist_3000_10.edges', 'ba_edgelist_3000_10'), ('./datasets/BA\\\\ba_edgelist_3000_20.edges', 'ba_edgelist_3000_20'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_2.edges', 'ba_edgelist_exp3_4000_2'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_4000_4.edges', 'ba_edgelist_exp1_4000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_6.edges', 'ba_edgelist_exp3_4000_6'), ('./datasets/BA_EXP\\\\ba_edgelist_exp3_4000_10.edges', 'ba_edgelist_exp3_4000_10'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_5000_4.edges', 'ba_edgelist_exp1_5000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_6000_4.edges', 'ba_edgelist_exp1_6000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_7000_4.edges', 'ba_edgelist_exp1_7000_4'), ('./datasets/BA_EXP\\\\ba_edgelist_exp1_8000_4.edges', 'ba_edgelist_exp1_8000_4')]\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "temp = get_ba_graph_paths()\n",
    "print(temp)\n",
    "list.sort(temp, key=getSortValue)\n",
    "print(temp)\n",
    "ba_mch_0 = getSubList(temp, 0, 3)\n",
    "ba_mch_1 = getSubList(temp, 1, 3)\n",
    "ba_mch_2 = getSubList(temp, 2, 3)\n",
    "print(len(ba_mch_0))\n",
    "print(len(ba_mch_1))\n",
    "print(len(ba_mch_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "di = {}\n",
    "di['BA_mch_0'] = [item[1] for item in ba_mch_0]\n",
    "di['BA_mch_0'].append('ChicagoRegional')\n",
    "di['BA_mch_0'].append('jazz')\n",
    "di['BA_mch_0'].append('ia-crime-moreno')\n",
    "di['BA_mch_0'].append('p2p-Gnutella04')\n",
    "\n",
    "di['BA_mch_1'] = [item[1] for item in ba_mch_1]\n",
    "di['BA_mch_1'].append('arenas-pgp')\n",
    "di['BA_mch_1'].append('LastFM')\n",
    "di['BA_mch_1'].append('sex')\n",
    "\n",
    "di['BA_mch_2'] = [item[1] for item in ba_mch_2]\n",
    "di['negin_mch'] = ['CA-GrQc', 'CA-HepTh', 'faa', 'facebook_combined', 'figeys', 'email', 'NS', 'Peh_edge', 'Stelzl', 'tvshow_edges', 'vidal', 'web-EPA']\n",
    "di['mhd_mch'] = [ 'maybe-PROTEINS-full']\n",
    "\n",
    "write_to_mch_json = False\n",
    "if write_to_mch_json:\n",
    "    with open('machine.json', 'w') as f:\n",
    "        json.dump(di, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA_mch_0 : \n",
      "1 . ba_edgelist_1000_4   process: [1000, 1000, 1000] / 1000 completed:  True  sorted: True\n",
      "2 . ba_edgelist_2000_4   process: [2000, 2000, 2000] / 2000 completed:  True  sorted: True\n",
      "###  3 . ba_edgelist_exp2_2000_10   process: [2009, 2009, 2009] / 2000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [20, 21, 29, 30, 204, 205, 206, 207, 208]\n",
      "4 . ba_edgelist_3000_4   process: [3004, 3004, 3004] / 3000 completed:  True  sorted: True\n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [32, 33, 34, 35]\n",
      "###  5 . ba_edgelist_3000_20   process: [1223, 1223, 1223] / 3000 completed:  False  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [778, 779, 780, 781, 788, 789, 790, 791, 822, 823, 824, 825, 826, 924, 925, 926, 927, 928, 1077, 1078, 1079, 1080, 1081]\n",
      "###  6 . ba_edgelist_exp3_4000_6   process: [4023, 4023, 4023] / 4000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [12, 16, 17, 108, 109, 110, 116, 117, 118, 155, 156, 157, 158, 270, 271, 272, 273, 274, 314, 315, 316, 317, 318]\n",
      "###  7 . ba_edgelist_exp1_6000_4   process: [6021, 6021, 6021] / 6000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [11, 12, 16, 114, 115, 116, 123, 124, 164, 165, 166, 167, 286, 287, 288, 289, 290, 331, 332, 333, 334]\n",
      "8 . ChicagoRegional   process: [0, 0, 0] / 12979 completed:  False  sorted: True\n",
      "9 . jazz   process: [0, 0, 0] / 198 completed:  False  sorted: True\n",
      "10 . ia-crime-moreno   process: [0, 0, 0] / 829 completed:  False  sorted: True\n",
      "11 . p2p-Gnutella04   process: [16, 16, 16] / 10876 completed:  False  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  19296  /  45882\n",
      "percentage:  42.055708120831696\n",
      "COMPLETED:  ['ba_edgelist_1000_4', 'ba_edgelist_2000_4', 'ba_edgelist_exp2_2000_10', 'ba_edgelist_3000_4', 'ba_edgelist_exp3_4000_6', 'ba_edgelist_exp1_6000_4']\n",
      "INCOMPLETED:  ['ba_edgelist_3000_20', 'ChicagoRegional', 'jazz', 'ia-crime-moreno', 'p2p-Gnutella04']\n",
      "-----------------\n",
      "BA_mch_1 : \n",
      "1 . ba_edgelist_1000_10   process: [1000, 1000, 1000] / 1000 completed:  True  sorted: True\n",
      "2 . ba_edgelist_exp2_2000_4   process: [2000, 2000, 2000] / 2000 completed:  True  sorted: True\n",
      "###  3 . ba_edgelist_2000_20   process: [1617, 1617, 1617] / 2000 completed:  False  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [898, 899, 900, 901, 902, 909, 910, 911, 912, 913, 914, 915, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388]\n",
      "4 . ba_edgelist_exp1_3000_4   process: [3000, 3000, 3000] / 3000 completed:  True  sorted: True\n",
      "###  5 . ba_edgelist_exp3_4000_2   process: [4018, 4018, 4018] / 4000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [209, 210, 211, 212, 213, 214, 215, 216, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358]\n",
      "6 . ba_edgelist_exp3_4000_10   process: [4004, 4004, 4004] / 4000 completed:  True  sorted: True\n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [49, 83, 84, 85]\n",
      "###  7 . ba_edgelist_exp1_7000_4   process: [2125, 2125, 2125] / 7000 completed:  False  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [1835, 1836, 1837, 1841, 1842, 1843, 1844, 1967, 1968, 1969, 1970]\n",
      "8 . arenas-pgp   process: [0, 0, 0] / 10680 completed:  False  sorted: True\n",
      "9 . LastFM   process: [68, 68, 68] / 7624 completed:  False  sorted: True\n",
      "10 . sex   process: [0, 0, 0] / 10106 completed:  False  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  17832  /  51410\n",
      "percentage:  34.685858782338066\n",
      "COMPLETED:  ['ba_edgelist_1000_10', 'ba_edgelist_exp2_2000_4', 'ba_edgelist_exp1_3000_4', 'ba_edgelist_exp3_4000_2', 'ba_edgelist_exp3_4000_10']\n",
      "INCOMPLETED:  ['ba_edgelist_2000_20', 'ba_edgelist_exp1_7000_4', 'arenas-pgp', 'LastFM', 'sex']\n",
      "-----------------\n",
      "BA_mch_2 : \n",
      "1 . ba_edgelist_1000_20   process: [0, 0, 0] / 1000 completed:  False  sorted: True\n",
      "###  2 . ba_edgelist_2000_10   process: [1352, 1352, 1352] / 2000 completed:  False  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [512, 513, 514, 515, 516, 517, 518, 519, 530, 531, 532, 533, 534, 535, 536, 781, 782, 783, 784, 785, 786, 787, 788, 789, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078]\n",
      "###  3 . ba_edgelist_exp2_2000_20   process: [2007, 2007, 2007] / 2000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [42, 43, 44, 121, 122, 123, 124]\n",
      "4 . ba_edgelist_3000_10   process: [0, 0, 0] / 3000 completed:  False  sorted: True\n",
      "###  5 . ba_edgelist_exp1_4000_4   process: [4010, 4010, 4010] / 4000 completed:  True  sorted: False ### \n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [46, 47, 48, 49, 50, 140, 141, 142, 143, 144]\n",
      "6 . ba_edgelist_exp1_5000_4   process: [5000, 5000, 5000] / 5000 completed:  True  sorted: True\n",
      "7 . ba_edgelist_exp1_8000_4   process: [2362, 2362, 2362] / 8000 completed:  False  sorted: True\n",
      "@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates:  [2002, 2003, 2004, 2005, 2010, 2011, 2012, 2126, 2127, 2128, 2252, 2253, 2254]\n",
      "SIM NUM NODES / TOTAL NUM NODES:  14731  /  25000\n",
      "percentage:  58.924\n",
      "COMPLETED:  ['ba_edgelist_exp2_2000_20', 'ba_edgelist_exp1_4000_4', 'ba_edgelist_exp1_5000_4']\n",
      "INCOMPLETED:  ['ba_edgelist_1000_20', 'ba_edgelist_2000_10', 'ba_edgelist_3000_10', 'ba_edgelist_exp1_8000_4']\n",
      "-----------------\n",
      "negin_mch : \n",
      "1 . CA-GrQc   process: [3759, 3759, 3759] / 5242 completed:  False  sorted: True\n",
      "2 . CA-HepTh   process: [1102, 1102, 1102] / 9877 completed:  False  sorted: True\n",
      "3 . faa   process: [1226, 1226, 1226] / 1226 completed:  True  sorted: True\n",
      "4 . facebook_combined   process: [2536, 2536, 2536] / 4039 completed:  False  sorted: True\n",
      "5 . figeys   process: [2239, 2239, 2239] / 2239 completed:  True  sorted: True\n",
      "6 . email   process: [1133, 1133, 1133] / 1133 completed:  True  sorted: True\n",
      "7 . NS   process: [1461, 1461, 1461] / 1461 completed:  True  sorted: True\n",
      "8 . Peh_edge   process: [2426, 2426, 2426] / 2426 completed:  True  sorted: True\n",
      "9 . Stelzl   process: [1706, 1706, 1706] / 1706 completed:  True  sorted: True\n",
      "10 . tvshow_edges   process: [147, 147, 147] / 3892 completed:  False  sorted: True\n",
      "11 . vidal   process: [0, 0, 0] / 3133 completed:  False  sorted: True\n",
      "12 . web-EPA   process: [0, 0, 0] / 4271 completed:  False  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  17735  /  40645\n",
      "percentage:  43.633903309140116\n",
      "COMPLETED:  ['faa', 'figeys', 'email', 'NS', 'Peh_edge', 'Stelzl']\n",
      "INCOMPLETED:  ['CA-GrQc', 'CA-HepTh', 'facebook_combined', 'tvshow_edges', 'vidal', 'web-EPA']\n",
      "-----------------\n",
      "mhd_mch : \n",
      "1 . maybe-PROTEINS-full   process: [0, 0, 0] / 43466 completed:  False  sorted: True\n",
      "SIM NUM NODES / TOTAL NUM NODES:  0  /  43466\n",
      "percentage:  0.0\n",
      "COMPLETED:  []\n",
      "INCOMPLETED:  ['maybe-PROTEINS-full']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "notsorted = [] \n",
    "noteq = []\n",
    "completed = []\n",
    "incomplete = []\n",
    "temp = False\n",
    "net_has_duplicate_name = []\n",
    "\n",
    "for mch in di.keys():\n",
    "    print(mch, \": \")\n",
    "    i = 0\n",
    "\n",
    "    t_=0\n",
    "    s_=0\n",
    "    for net in di[mch]:\n",
    "        has_duplicates = get_has_duplicates(net)\n",
    "        temp = False\n",
    "        i+=1\n",
    "        simNodes = getSimNumNodes(net)\n",
    "        totalNumNodes = getTotalNumNodes(net)\n",
    "        t_+=totalNumNodes\n",
    "        s_+=simNodes[0]\n",
    "        sortd = sorted(get_previously_sim_values(get_sir_graph_paths(net)[0])) ==get_previously_sim_values(get_sir_graph_paths(net)[0])\n",
    "        if simNodes[0] >= totalNumNodes:\n",
    "            temp = True\n",
    "            completed.append(net)\n",
    "        else: \n",
    "            incomplete.append(net)\n",
    "        if not sortd :\n",
    "            notsorted.append(net)\n",
    "            print('### ', i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd, '### ')\n",
    "        elif not( simNodes[0]==simNodes[1] and simNodes[1]==simNodes[2]) :\n",
    "            noteq.append(net)\n",
    "            print('$$$ ', i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd, ('$$$ '))\n",
    "        else:\n",
    "            print( i, \".\", net, \"  process:\", simNodes, '/',  totalNumNodes, 'completed: ',temp,  ' sorted:', sortd)\n",
    "        if has_duplicates:\n",
    "            net_has_duplicate_name.append(net)\n",
    "            print(\"@@@@@@@@@@@@ has duplicate nodes in the node list, duplicates: \", get_duplicates(net))\n",
    "    print(\"SIM NUM NODES / TOTAL NUM NODES: \",s_, ' / ', t_)\n",
    "    print(\"percentage: \",(s_/ t_)*100)\n",
    "    print(\"COMPLETED: \", completed)\n",
    "    print(\"INCOMPLETED: \", incomplete)\n",
    "\n",
    "    completed=[]\n",
    "    incomplete = []\n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Output:\n",
    "BA_mch_0 : \n",
    "1. ba_edgelist_1000_4   process: [0, 0, 0] / 1000  sorted: True\n",
    "2. ba_edgelist_2000_4   process: [0, 0, 0] / 2000  sorted: True\n",
    "3. ba_edgelist_exp2_2000_10   process: [481, 481, 481] / 2000  sorted: True\n",
    "4. ba_edgelist_3000_4   process: [315, 315, 315] / 3000  sorted: True\n",
    "5. ba_edgelist_3000_20   process: [0, 0, 0] / 3000  sorted: True\n",
    "6. ba_edgelist_exp3_4000_6   process: [913, 913, 913] / 4000  sorted: True\n",
    "7. ba_edgelist_exp1_6000_4   process: [604, 604, 604] / 6000  sorted: True\n",
    "\n",
    "\n",
    "##### SIM NUM NODES/TOTAL NUM NODES:  2313  /  21000\n",
    "##### percentage:  11.014285714285714\n",
    "##### COMPLETED:  []\n",
    "-----------------\n",
    "BA_mch_1 : \n",
    "1. ba_edgelist_1000_10   process: [488, 488, 488] / 1000  sorted: True\n",
    "2. ba_edgelist_exp2_2000_4   process: [0, 0, 0] / 2000  sorted: True\n",
    "3. ba_edgelist_2000_20   process: [0, 0, 0] / 2000  sorted: True\n",
    "4. ba_edgelist_exp1_3000_4   process: [320, 320, 320] / 3000  sorted: True\n",
    "5. ba_edgelist_exp3_4000_2   process: [906, 906, 906] / 4000  sorted: True\n",
    "6. ba_edgelist_exp3_4000_10   process: [241, 241, 241] / 4000  sorted: True\n",
    "7. ba_edgelist_exp1_7000_4   process: [138, 138, 138] / 7000  sorted: True\n",
    "\n",
    "\n",
    "##### SIM NUM NODES / TOTAL NUM NODES:  2093  /  23000\n",
    "##### percentage:  9.1\n",
    "##### COMPLETED:  []\n",
    "-----------------\n",
    "BA_mch_2 : \n",
    "1. ba_edgelist_1000_20   process: [0, 0, 0] / 1000  sorted: True\n",
    "2. ba_edgelist_2000_10   process: [0, 0, 0] / 2000  sorted: True\n",
    "3. ba_edgelist_exp2_2000_20   process: [500, 500, 500] / 2000  sorted: True\n",
    "4. ba_edgelist_3000_10   process: [161, 161, 161] / 3000  sorted: True\n",
    "5. ba_edgelist_exp1_4000_4   process: [249, 249, 249] / 4000  sorted: True\n",
    "6. ba_edgelist_exp1_5000_4   process: [741, 741, 741] / 5000  sorted: True\n",
    "7. ba_edgelist_exp1_8000_4   process: [122, 122, 122] / 8000  sorted: True\n",
    "\n",
    "##### SIM NUM NODES / TOTAL NUM NODES:  1773  /  25000\n",
    "##### percentage:  7.092\n",
    "##### COMPLETED:  []\n",
    "-----------------\n",
    "negin_mch : \n",
    "1. CA-GrQc   process: [1015, 1015, 1015] / 5242  sorted: True\n",
    "2. CA-HepTh   process: [411, 411, 411] / 9877  sorted: True\n",
    "3. faa   process: [1226, 1226, 1226] / 1226  sorted: True\n",
    "4. facebook_combined   process: [1460, 1460, 1460] / 4039  sorted: True\n",
    "5. figeys   process: [1946, 1946, 1946] / 2239  sorted: True\n",
    "6. email   process: [1133, 1133, 1133] / 1133  sorted: True\n",
    "7. NS   process: [1461, 1461, 1461] / 1461  sorted: True\n",
    "8. Peh_edge   process: [967, 967, 967] / 2426  sorted: True\n",
    "9. Stelzl   process: [980, 980, 980] / 1706  sorted: True\n",
    "10. tvshow_edges   process: [0, 0, 0] / 3892  sorted: True\n",
    "11. vidal   process: [0, 0, 0] / 3133  sorted: True\n",
    "12. web-EPA   process: [0, 0, 0] / 4271  sorted: True\n",
    "\n",
    "##### SIM NUM NODES / TOTAL NUM NODES:  10599  /  40645\n",
    "##### percentage:  26.0770082420962\n",
    "##### COMPLETED:  ['faa', 'email', 'NS']\n",
    "-----------------\n",
    "mhd_mch : \n",
    "1. ChicagoRegional   process: [1385, 1385, 1385] / 12979  sorted: True\n",
    "2. jazz   process: [198, 198, 198] / 198  sorted: True\n",
    "3. ia-crime-moreno   process: [829, 829, 829] / 829  sorted: True\n",
    "4. arenas-pgp   process: [1675, 1675, 1675] / 10680  sorted: True\n",
    "5. LastFM   process: [1859, 1859, 1859] / 7624  sorted: True\n",
    "6. maybe-PROTEINS-full   process: [322, 322, 322] / 43466  sorted: True\n",
    "7. p2p-Gnutella04   process: [1134, 1134, 1134] / 10876  sorted: True\n",
    "8. sex   process: [0, 0, 0] / 10106  sorted: True\n",
    "   \n",
    "##### SIM NUM NODES / TOTAL NUM NODES:  7402  /  96758\n",
    "##### percentage:  7.650013435581554\n",
    "##### COMPLETED:  ['jazz', 'ia-crime-moreno']\n",
    "-----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    for net in notsorted:\n",
    "        sir_paths = get_sir_graph_paths(net)\n",
    "        i = 0\n",
    "        for path in sir_paths:\n",
    "            i+=1\n",
    "            df = pd.read_csv(path)\n",
    "            df_sorted = df.sort_values(by='Node')\n",
    "            df_sorted.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE full duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_full_duplicates(net_name):\n",
    "    sir_paths = get_sir_graph_paths(net_name)\n",
    "    sir_graph_path = sir_paths[0]\n",
    "    try:\n",
    "        # Load the CSV file with explicit data types\n",
    "        df = pd.read_csv(sir_graph_path, dtype={'Node': int, 'SIR': float, 'Infected_sum': int})\n",
    "        \n",
    "        # Remove fully duplicated rows, ensuring all columns are considered\n",
    "        df_cleaned = df.drop_duplicates(subset=['Node', 'SIR', 'Infected_sum'])\n",
    "        \n",
    "        # Overwrite the original CSV file with the cleaned data\n",
    "        df_cleaned.to_csv(sir_graph_path, index=False)\n",
    "        \n",
    "        print(f\"File {sir_graph_path} updated, fully duplicated rows removed.\")\n",
    "        \n",
    "    except OSError as e:\n",
    "        print(\"Error reading file:\", e)\n",
    "\n",
    "if False:\n",
    "    for net in net_has_duplicate_name:\n",
    "        print(\"net: \", net)\n",
    "        remove_full_duplicates(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
